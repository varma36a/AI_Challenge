Nice — I’ll recreate the full step-by-step plan we followed **plus every mistake you hit and how we fixed it**. I’ll order it as a chronological runbook you can reuse for future projects (so you — or a teammate — don’t repeat the same hiccups).

I’ll keep each step actionable (commands / UI clicks / exact file snippets). Where you previously hit errors, I’ll name the error, why it happened, and the precise fix you used.

---

# End-to-end action plan (what we did) — building the Week-21 solution with **Azure OpenAI + Azure ML**

> Goal: run a FastAPI app that orchestrates Azure OpenAI (LLM) + an Azure ML real-time endpoint (AutoML model) to answer `POST /api/chat` requests, with tool calling, fallback logic, and production readiness.

---

## 0) Project scaffold & local environment

What you should have:

* Project root (e.g. `AI_Challenge` or `AIChallenge`) with `app/` (FastAPI app), `tools.py`, `llm.py`, `main.py`.
* A Python venv in project root named `.venv`.
* `.env` file to hold secrets (never commit to Git).
* `requirements.txt`.

Key commands:

```bash
cd /path/to/AI_Challenge
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt   # or pip install fastapi uvicorn openai httpx python-dotenv
```

**Mistakes you made & fixes**

* Mistake: activated wrong environment / `.venv` missing. Fix: create `.venv` and `source .venv/bin/activate`.
* Mistake: `uvicorn` not installed inside venv → `zsh: command not found: uvicorn`. Fix: `pip install uvicorn` (inside venv).

---

## 1) Azure OpenAI setup (for LLM)

Create Azure OpenAI resource in Azure portal or AI Foundry per your org rules.

Important env vars:

```env
AZURE_OPENAI_ENDPOINT=https://<your-aoai-name>.<region>.openai.azure.com
AZURE_OPENAI_API_KEY=<your-aoai-key>
AZURE_OPENAI_DEPLOYMENT=<your-deployed-chat-model-name>   # e.g. gpt-5-mini-deploy
OPENAI_API_VERSION=2024-05-01-preview
```

What we used in code: instantiate `AzureOpenAI(api_key=..., azure_endpoint=..., api_version=...)`.

**Mistakes you made & fixes**

* Mistake: Missing `OPENAI_API_VERSION` or not passing `api_version` to `AzureOpenAI` → raised `ValueError: Must provide either api_version or OPENAI_API_VERSION`.
  Fix: set `OPENAI_API_VERSION=2024-05-01-preview` in `.env` or pass `api_version` to constructor.

---

## 2) App code: tool-calling loop, robustly handling tool calls

Goal: LLM may call tools. We must:

1. Send system/dev/user messages.
2. Call `client.chat.completions.create(...)` with `tools` schema and `tool_choice="auto"`.
3. If the assistant returns `tool_calls`, execute them (run `predict_customer`, `get_stat`, etc.), append `tool` messages back to conversation, continue loop until assistant returns final content.
4. Parse final assistant content (JSON or markdown) and return.

Minimal robust loop (in `app/main.py`) — **use this exact flow** (we used it successfully):

```python
# simplified excerpt (synchronous style used in your app)
messages = [
    {"role":"system","content":SYSTEM},
    {"role":"developer","content":DEVELOPER},
    {"role":"user","content":req.message},
]

actions_result = []

while True:
    resp = client.chat.completions.create(
        model=AZURE_OPENAI_DEPLOYMENT,
        messages=messages,
        tools=tool_schemas(),
        tool_choice="auto",
    )
    msg = resp.choices[0].message

    if getattr(msg, "tool_calls", None):
        # add assistant tool call to messages
        messages.append({"role":"assistant","tool_calls":[tc.model_dump() for tc in msg.tool_calls]})
        for tc in msg.tool_calls:
            name = tc.function.name
            args = json.loads(tc.function.arguments or "{}")
            # call local tool and return result to model
            if name == "predict_customer":
                result = predict_customer(args)
            elif name == "get_stat":
                result = get_stat(args.get("key"))
            else:
                result = {"error":"unknown tool"}
            actions_result.append({"tool":name,"result":result})
            messages.append({
                "role":"tool",
                "tool_call_id": tc.id,
                "name": name,
                "content": json.dumps(result),
            })
        continue

    # no tool calls: final assistant content
    content = msg.content or ""
    try:
        plan = json.loads(content)
        intent = plan.get("intent","answer")
        answer_md = plan.get("answer_md","")
    except Exception:
        intent = "answer"
        answer_md = content

    return {"intent": intent, "answer_md": answer_md, "actions_result": actions_result}
```

**Mistakes you made & fixes**

* Mistake: earlier code tried `json.loads(msg.content)` even when `msg.content` was `None` → error `JSON object must be str, bytes or bytearray, not NoneType`.
  Fix: implement tool-call loop above and only `json.loads` when `msg.content` exists.
* Mistake: imported `run_llm_turn` when code no longer had it → `ImportError: cannot import name 'run_llm_turn'`. Fix: update imports in `main.py` to import required symbols (`client, AZURE_OPENAI_DEPLOYMENT, SYSTEM, DEVELOPER, tool_schemas`) and remove stale imports.

---

## 3) Tools: `predict_customer` and `get_stat`

Make `predict_customer` robust for three cases:

* If no `AML_SCORING_URI` defined → run a **local mock fallback** (so demo works).
* If `AML_SCORING_URI` defined → post to remote endpoint using the exact payload shape the endpoint expects (Consume tab).
* Diagnostics: if non-2xx, return `status`, `request_id`, `error_text`, and `sent_body` (do **not** raise), so the chatbot can explain errors.

Recommended `predict_customer` (drop-in):

```python
import os, json, httpx
from .schemas import CustomerFeatures

def predict_customer(payload: dict):
    features = CustomerFeatures(**payload)  # pydantic validate
    AML_SCORING_URI = os.getenv("AML_SCORING_URI","").strip()
    AML_API_KEY = os.getenv("AML_API_KEY","").strip()
    AML_DEPLOYMENT = os.getenv("AML_DEPLOYMENT","").strip()

    if not AML_SCORING_URI:
        # fallback heuristic (mock)
        total_delay = float(features.DepDelay) + float(features.ArrDelay)
        score = 0.5 + (features.SeatComfort-3)*0.08 + (features.Cleanliness-3)*0.06
        score += (4 - min(total_delay/60,4))*0.05
        score = max(0.0, min(1.0, score))
        label = "Satisfied" if score >= 0.5 else "Dissatisfied"
        return {"label": label, "proba": round(score if label=="Satisfied" else 1-score,4), "source":"mock"}

    headers = {"Content-Type":"application/json","Accept":"application/json"}
    if AML_API_KEY:
        headers["Authorization"] = f"Bearer {AML_API_KEY}"
    if AML_DEPLOYMENT:
        headers["azureml-model-deployment"] = AML_DEPLOYMENT

    columns = list(payload.keys())
    values = [payload[k] for k in columns]
    candidate_bodies = [
        {"input_data": {"columns": columns, "data": [values]}},  # MLflow/AutoML typical
        {"inputs": [{"name":"input-0","columns": columns,"data":[values]}]},  # alt
        {"input_data":[payload]},  # custom handler possibility
    ]

    last = None
    with httpx.Client(timeout=30.0) as client:
        for body in candidate_bodies:
            r = client.post(AML_SCORING_URI, headers=headers, json=body)
            if r.status_code // 100 == 2 and (r.text or "").strip():
                try:
                    return r.json()
                except Exception:
                    return {"status": r.status_code, "text": (r.text or "")[:2000]}
            last = {
                "status": r.status_code,
                "request_id": r.headers.get("x-ms-request-id") or r.headers.get("x-request-id"),
                "error_text": (r.text or "")[:2000],
                "sent_body": body
            }
    return {"error":"AML endpoint call failed","hint":"Match the Consume tab exactly","details": last}
```

**Mistakes you made & fixes**

* Mistake: Sent flat JSON (feature dict) to endpoint → got `424 Failed Dependency` and `"A value is not provided for the 'input_data' parameter."`
  Fix: send `{"input_data": {"columns":[...], "data":[[...] ]}}` or the exact sample shown in the endpoint Consume tab.
* Mistake: using `r.json()` without checking response code → `JSONDecodeError` or `HTTPStatusError`. Fix: inspect `r.status_code` and return diagnostic info instead of raising.

---

## 4) Azure ML AutoML training → Deploy as Real-time endpoint

Steps we followed:

1. Upload dataset in **Azure ML Studio > Data > Create from local**.
2. Start **Automated ML job** (Task = Classification, Target = `Satisfaction`).

   * Validation: Automatic or Train/Validation split
   * Primary metric: Accuracy (or AUC if imbalance)
   * Limits: set max runtime (30–60min) and max iterations (e.g., 20).
3. Wait for job to complete → open **Best model** (VotingEnsemble MLflow).
4. Deploy:

   * You can deploy in UI (Deploy → Real-time endpoint) or via CLI.
   * We created endpoint then a deployment using `deployment.yml` (CLI), because UI or CLI both work.
   * Example `deployment.yml`:

```yaml
$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json
name: blue
endpoint_name: custsat-endpoint-01
model: azureml:lovingnapkindn047:1
instance_type: Standard_F2s_v2
instance_count: 1
```

Deploy command (if using CLI):

```bash
az ml online-endpoint create -g rohit36a-rg -w ws1 -n custsat-endpoint-01   # only if endpoint missing
az ml online-deployment create -g rohit36a-rg -w ws1 -f deployment.yml --all-traffic
```

5. When Healthy → **Endpoints > Real-time endpoints > [endpoint] > Consume** to copy:

   * REST endpoint (scoring URI, ends with `/score`) → `AML_SCORING_URI`
   * Primary key → `AML_API_KEY`
   * If required header `azureml-model-deployment` appears, set `AML_DEPLOYMENT` accordingly
   * Also copy example request JSON from Consume tab.

**Mistakes you made & fixes**

* Mistake: Tried to create deployment without required **resource providers** registered → `SubscriptionNotRegistered`.
  Fix: register providers at subscription scope:

  ```bash
  az provider register --namespace Microsoft.MachineLearningServices
  az provider register --namespace Microsoft.Compute
  az provider register --namespace Microsoft.Storage
  az provider register --namespace Microsoft.ContainerRegistry
  az provider register --namespace Microsoft.KeyVault
  az provider register --namespace Microsoft.Network
  az provider register --namespace Microsoft.ManagedIdentity
  az provider register --namespace Microsoft.OperationalInsights
  az provider register --namespace Microsoft.Insights
  ```

  (If using PowerShell, run the same commands; if you lack permissions, ask admin to register).
* Mistake: endpoint created but no deployment (empty) — you needed `deployment.yml` + `az ml online-deployment create`. Fix: create the YAML and deploy.
* Mistake: tried inline CLI old style; CLI v2 requires YAML file. Fix: create `deployment.yml` and run `az ml online-deployment create -f deployment.yml`.

---

## 5) Getting keys and wiring `.env`

After deployment success, fetch strings:

CLI:

```bash
# scoring URI (paste into .env as AML_SCORING_URI)
az ml online-endpoint show -g rohit36a-rg -w ws1 -n custsat-endpoint-01 --query scoring_uri -o tsv

# api key
az ml online-endpoint get-credentials -g rohit36a-rg -w ws1 -n custsat-endpoint-01 --query primaryKey -o tsv
```

Then in your project `.env`:

```env
# Azure OpenAI
AZURE_OPENAI_ENDPOINT=https://<your-aoai>.openai.azure.com/
AZURE_OPENAI_API_KEY=<aoai-key>
AZURE_OPENAI_DEPLOYMENT=<aoai-deployment-name>
OPENAI_API_VERSION=2024-05-01-preview

# Azure ML
AML_SCORING_URI=https://custsat-endpoint-01.<region>.inference.ml.azure.com/score
AML_API_KEY=<primary-key-value>
AML_DEPLOYMENT=blue   # optional; add if Consume tab demands header
```

**Mistakes you made & fixes**

* Mistake: used an OpenAI-style URL for `AML_SCORING_URI` (e.g., `.openai.azure.com`) → endpoint returned empty or 404. Fix: use the AML `inference.ml.azure.com/score` URL from the endpoint Consume tab.
* Mistake: committed `.env` secrets to Git → GitHub blocked push and secret scanning triggered. Fixes:

  * Remove secret from commits (`git reset --soft HEAD~1` / `git restore --staged .env` / or rewrite history with `git filter-repo`), add `.env` to `.gitignore`.
  * **Rotate** the leaked keys in Azure (regenerate).

---

## 6) Local run of FastAPI

Start server:

```bash
source .venv/bin/activate
uvicorn app.main:app --reload --port 8000
```

Local test HTTP:

```bash
curl -X POST http://127.0.0.1:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message":"Predict satisfaction for Age=32, Gender=Male, TravelCategory=Business,..."}'
```

**Common runtime issues & fixes**

* `ImportError: cannot import name 'run_llm_turn'` → stale imports. Fix: remove or re-add function.
* `openai.BadRequestError` about `response_format` schema → we removed `response_format` usage (or fully define strict JSON schema with `additionalProperties: false` everywhere). Fix: remove `response_format` and rely on tool calls.
* `Bad LLM output: JSON must be str not NoneType` → handle `tool_calls` and `msg.content` None as explained.

---

## 7) Git & secret handling (what happened + steps to recover)

You attempted a Git push but GitHub rejected push due to secret scanning. We saw:

```
remote rejected main -> main (push declined due to repository rule violations)
```

How to clean up:

1. Stop: do not push secrets again.
2. Remove secrets from local history:

   * If secret in last commit:

     ```bash
     git reset --soft HEAD~1
     git restore --staged .env
     git commit -m "remove .env from commit"
     ```
   * Add `.env` to `.gitignore`
   * If secret exists deeper in history, run `git filter-repo`:

     ```bash
     pip install git-filter-repo
     git filter-repo --path .env --invert-paths
     ```

     Then force push: `git push origin main --force`
3. Rotate leaked keys in Azure (regenerate `AML_API_KEY` and `AZURE_OPENAI_API_KEY`).
4. Use GitHub secret management for CI rather than committing `.env`.

---

## 8) VS Code formatting (small bookkeeping)

* Set Black as Python formatter: `Preferences: Open Settings (JSON)` and add:

```json
{
  "editor.formatOnSave": true,
  "[python]": {
    "editor.defaultFormatter": "ms-python.black-formatter"
  },
  "python.formatting.provider": "none"
}
```

* Mistake: Prettier was active and tried to format Python. Fix: set default formatter per-language as above.

---

## 9) Final verification checklist (run this before demo)

1. `.venv` active, `uvicorn` runs without errors.
2. `.env` present locally (not committed); env vars set:

   * `AZURE_OPENAI_*`, `OPENAI_API_VERSION`, `AML_SCORING_URI`, `AML_API_KEY`, `AML_DEPLOYMENT` (if required).
3. FastAPI `POST /api/chat` returns a JSON with `answer_md` and `actions_result` and does not raise exceptions.
4. A successful direct curl to `AML_SCORING_URI` (Consume sample payload) returns a JSON prediction.
5. Git repo cleaned (no secrets), `.gitignore` updated, keys rotated if leaked.
6. App logs show tool calls executed and model responses returned (or fallback used).

---

## Appendix — Short patch snippets you used

### `predict_customer` (robust/pasteable)

(Already provided above — use that exact function.)

### Safe POST wrapper: return diagnostics instead of raising:

```python
r = client.post(AML_SCORING_URI, headers=headers, json=body)
if r.status_code // 100 != 2:
    return {
        "status": r.status_code,
        "request_id": r.headers.get("x-ms-request-id") or r.headers.get("x-request-id"),
        "error_text": (r.text or "")[:2000],
        "sent_body": body
    }
try:
    return r.json()
except Exception:
    return {"status": r.status_code, "text": (r.text or "")[:2000]}
```

### `deployment.yml` we used

```yaml
$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json
name: blue
endpoint_name: custsat-endpoint-01
model: azureml:lovingnapkindn047:1
instance_type: Standard_F2s_v2
instance_count: 1
```

---

## Quick timeline of *your* exact mistakes (condensed)

1. Didn’t activate `.venv` → uvicorn command not found. Fix: create/activate venv and `pip install` dependencies.
2. Missing `OPENAI_API_VERSION` → `ValueError`. Fix: set env var or pass `api_version`.
3. App crashed on tool calls because `msg.content` was `None` and code tried `json.loads` → fix: implement tool-calling loop.
4. Used wrong payload when calling AML (flat dict) → got `424` + `"input_data parameter missing"` → fix: send `input_data.columns/data` shape (or match Consume tab).
5. Tried to deploy online endpoint but subscription lacked providers → `SubscriptionNotRegistered` → fix: register required providers or ask admin.
6. Endpoint existed but no deployment → created `deployment.yml` and ran `az ml online-deployment create -f deployment.yml`.
7. Committed `.env` with secrets → GitHub blocked push / secret-scan triggered → fix: remove from commits, add `.gitignore`, rotate keys.

---

If you want, I’ll:

* Produce a single **one-page README** you can drop into the repo summarizing the exact commands and `.env` template (I can create that file content for you), or
* Provide a **postmortem checklist** for your team to avoid each mistake (e.g., preflight script that checks env vars and registered resource providers), or
* Paste the exact **patched `main.py` and `tools.py`** content ready to copy/paste into your project.

Which of those would you like me to produce next?
